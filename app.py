"""
Main FastAPI Application Entry Point
Graph-Based Multi-Agent AI Chat Onboarding System
"""

import os
import sys
import io
import logging
from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from database.state_manager import StateManager
from typing import Optional

# Fix Unicode encoding issues on Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Load environment variables
load_dotenv()

# Configure logging with UTF-8 encoding
os.makedirs('logs', exist_ok=True)

stream_handler = logging.StreamHandler(sys.stdout)
stream_handler.setStream(sys.stdout)

logging.basicConfig(
    level=os.getenv('LOG_LEVEL'),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/app.log', encoding='utf-8'),
        stream_handler
    ]
)

logger = logging.getLogger(__name__)

# Import our modules
from agents.supervised_onboarding_with_llm import SupervisedOnboardingSystemWithLLM


# ==========================================
# Pydantic Models for Request/Response
# ==========================================

class ChatRequest(BaseModel):
    """Chat request model"""
    message: str = Field(..., min_length=1, max_length=1000, description="User message")
    session_id: str = Field(..., min_length=10, description="Session UUID")
    
    class Config:
        json_schema_extra = {
            "example": {
                "message": "Hello, I want to start onboarding",
                "session_id": "550e8400-e29b-41d4-a716-446655440000"
            }
        }


class ChatResponse(BaseModel):
    """Chat response model"""
    session_id: str
    response: str
    status: str
    current_step: Optional[str] = None
    next_agent: Optional[str] = None
    entity_id: Optional[str] = None


class HealthResponse(BaseModel):
    """Health check response model"""
    status: str
    service: str
    version: str
    llm_provider: str
    features: dict
    endpoints: dict


# ==========================================
# FastAPI Application Setup
# ==========================================

def create_app() -> FastAPI:
    """Create and configure FastAPI application"""
    
    app = FastAPI(
        title="LLM-Powered Multi-Agent Onboarding",
        description="Graph-Based Multi-Agent AI Chat Onboarding System",
        version="4.0.0",
        docs_url="/docs",
        redoc_url="/redoc"
    )
    
    # Enable CORS
    origins = os.getenv('CORS_ORIGINS', '*').split(',')
    app.add_middleware(
        CORSMiddleware,
        allow_origins=origins,
        allow_credentials=True,
        allow_methods=["GET", "POST"],
        allow_headers=["*"],
    )
    
    # Note: StateManager pool will be initialized in startup event
    # (StateManager is now class-based with async initialization)
    
    # Initialize LLM-powered supervised system
    supervisor = SupervisedOnboardingSystemWithLLM()
    
    # ==========================================
    # API Endpoints
    # ==========================================
    
    @app.get('/health', response_model=HealthResponse, tags=["System"])
    async def health_check():
        """
        Health check endpoint
        
        Returns service status and configuration information.
        """
        return {
            'status': 'healthy',
            'service': 'llm-powered-multi-agent-onboarding',
            'version': '4.0.0',
            'llm_provider': 'Google Gemini',
            'features': {
                'llm_powered': True,
                'intelligent_routing': True,
                'data_extraction': True,
                'langgraph': True,
                'multi_agent': True,
                'sequential_processing': True
            },
            'endpoints': {
                'chat': '/chat',
                'visualization': '/graph/visualize',
                'health': '/health',
                'docs': '/docs'
            }
        }
    
    @app.post('/chat', response_model=ChatResponse, tags=["Chat"])
    async def chat_endpoint(chat_request: ChatRequest):
        """
        Main chat endpoint - Uses LLM-powered supervised onboarding
        
        **Request Body:**
        - message: User's message (1-1000 characters)
        - session_id: UUID generated by client (min 10 characters)
        
        **Response:**
        - Returns AI response and session state
        """
        try:
            user_message = chat_request.message.strip()
            session_id = chat_request.session_id.strip()
            
            logger.info(f"üì® Chat request - Session: {session_id[:8]}...")
            
            # Load or create session state using StateManager (async)
            session_state = await StateManager.load_state(session_id)
            
            if session_state.get('current_step') == 'welcome':
                # New session
                logger.info(f"üÜï New session: {session_id[:8]}...")
                await StateManager.save_state(session_id, {
                    'session_id': session_id,
                    'current_step': 'welcome',
                    'status': 'active'
                })
            else:
                # Existing session
                entity_id = session_state.get('entity_id')
                logger.info(f"‚ôªÔ∏è Existing session: {session_id[:8]}... (entity_id: {entity_id})")
            
            # Process message through LLM-powered supervisor
            result = supervisor.process_onboarding(user_message, session_id)
            
            # Ensure session_id is in response
            if 'session_id' not in result:
                result['session_id'] = session_id
            
            logger.info(f"‚úÖ Response sent for session: {session_id[:8]}...")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error in chat endpoint: {str(e)}", exc_info=True)
            raise HTTPException(
                status_code=500,
                detail={
                    'error': 'Internal server error',
                    'message': 'An unexpected error occurred. Please try again.',
                    'status': 'error',
                    'session_id': chat_request.session_id
                }
            )
    
    @app.get('/graph/visualize', tags=["Visualization"])
    async def visualize_graph():
        """
        Get LLM-powered workflow graph visualization
        
        Returns PNG image of the agent workflow graph.
        """
        try:
            png_data = supervisor.get_graph_visualization()
            if png_data:
                return StreamingResponse(
                    io.BytesIO(png_data),
                    media_type='image/png'
                )
            else:
                raise HTTPException(status_code=500, detail='Visualization not available')
        except Exception as e:
            logger.error(f"Error generating visualization: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={'error': 'Failed to generate visualization', 'message': str(e)}
            )
    
    return app


# ==========================================
# Application Startup
# ==========================================

app = create_app()


@app.on_event("startup")
async def startup_event():
    """Run on application startup - Initialize async database pool"""
    try:
        # Initialize asyncpg connection pool
        await StateManager.initialize(enable_cache=True)
        logger.info("‚úÖ Async PostgreSQL connection pool initialized successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to initialize PostgreSQL pool: {e}")
        logger.error("Check your .env file and database configuration")
        # Note: FastAPI will still start, but with errors logged


@app.on_event("shutdown")
async def shutdown_event():
    """Run on application shutdown - Close async database pool"""
    try:
        await StateManager.close()
        logger.info("‚úÖ PostgreSQL connection pool closed")
    except Exception as e:
        logger.error(f"‚ùå Error closing database pool: {e}")
    logger.info("üõë Application shutdown complete")


# ==========================================
# Run Application
# ==========================================

if __name__ == '__main__':
    import uvicorn
    
    uvicorn.run(
        "app:app",
        host='0.0.0.0',
        port=5000,
        reload=os.getenv('FLASK_DEBUG', 'false').lower() == 'true',
        log_level=os.getenv('LOG_LEVEL', 'info').lower()
    )